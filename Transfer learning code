# Import the necessary libraries
import os
import numpy as np
import pandas as pd
import multiprocessing
import tensorflow as tf
import matplotlib
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras import backend as K
from keras.models import load_model
import matplotlib.pyplot as plt
from scipy.stats import pearsonr
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import warnings
warnings.filterwarnings('ignore', category=FutureWarning)

# Environment Setup
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"
num_cores = multiprocessing.cpu_count()
print("Number of CPU cores:", num_cores)
os.environ["OMP_NUM_THREADS"] = "20"
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')
        tf.config.experimental.set_memory_growth(gpus[0], True)
        print('GPU is being used')
    except RuntimeError as e:
        print(e)

# Load Data
data = pd.read_csv('sandy_1992.csv', header=0, parse_dates=[0])
DateTime = data[['Date', 'Time (GMT)']]
data = data.drop(columns=['Date', 'Time (GMT)'])
location = "datsandy1992"

# Preprocess Data
X = data.iloc[:, :-1]
y = data.iloc[:, -1]

def normalize_data(X):
    scaler = MinMaxScaler()
    X_norm = scaler.fit_transform(X)
    return X_norm, scaler

X_norm, scaler = normalize_data(X)
scaler_y = MinMaxScaler(feature_range=(0, 1))
y_reshaped = y.values.reshape(-1, 1)
y_scaled = scaler_y.fit_transform(y_reshaped)

def create_rnn_dataset(X_norm, y_scaled, lookback):
    X, y = [], []
    for i in range(len(X_norm) - lookback):
        X.append(X_norm[i:(i + lookback)])
        y.append(y_scaled[(i + lookback)])
    return np.array(X), np.array(y)

class CustomAttentionLayer(tf.keras.layers.Layer):
    def __init__(self, emphasis_factor=1.5, **kwargs):
        self.emphasis_factor = emphasis_factor
        super(CustomAttentionLayer, self).__init__(**kwargs)

    def get_config(self):
        config = super(CustomAttentionLayer, self).get_config()
        config.update({"emphasis_factor": self.emphasis_factor})
        return config

    def build(self, input_shape):
        self.W = self.add_weight(name='att_weight',
                                 shape=(input_shape[-1], 1),
                                 initializer='glorot_uniform',
                                 trainable=True)
        self.b = self.add_weight(shape=(1,),
                                 initializer='zeros',
                                 trainable=True,
                                 name='bias')
        super(CustomAttentionLayer, self).build(input_shape)

    def call(self, x):
        e = K.tanh(K.dot(x, self.W) + self.b)
        # Compute attention scores
        a = K.softmax(e, axis=1)

        # Identify top 10% values
        k_value = tf.cast(tf.cast(tf.shape(a)[1], tf.float32) * 0.1, tf.int32)
        top_10_percent_idx = tf.math.top_k(a, k=tf.math.maximum(1, k_value))[1]
        mask = tf.reduce_any(tf.equal(tf.range(tf.shape(a)[1])[None, :], top_10_percent_idx[:, :, None]), axis=1)

        # Apply emphasis
        emphasized_a = tf.where(mask, a * self.emphasis_factor, a)
        output = x * emphasized_a

        summed_output = K.sum(output, axis=1, keepdims=True)
        return summed_output

    def compute_output_shape(self, input_shape):
        return input_shape[0], input_shape[-1]
    
def kge(y_true, y_pred):
    """
    Compute Kling-Gupta Efficiency (KGE).
    """
    r, _ = pearsonr(y_true, y_pred)
    alpha = np.std(y_pred) / np.std(y_true)
    beta = np.mean(y_pred) / np.mean(y_true)
    
    return 1 - np.sqrt((r - 1)**2 + (alpha - 1)**2 + (beta - 1)**2)

def nse(y_true, y_pred):
    """
    Compute Nash-Sutcliffe Efficiency (NSE).
    """
    numerator = np.sum((y_true - y_pred) ** 2)
    denominator = np.sum((y_true - np.mean(y_true)) ** 2)
    return 1 - (numerator / denominator)

def amb(y_true, y_pred):
    """
    Compute Absolute Mean Bias.
    """
    return np.mean(np.abs(y_true - y_pred))

# Metrics and visualization
def willmott_skill(y_true, y_pred):
    obs_mean = np.mean(y_true)
    numerator = np.sum((y_true - y_pred) ** 2)
    denominator = np.sum((np.abs(y_pred - obs_mean) + np.abs(y_true - obs_mean)) ** 2)
    d = 1 - (numerator / denominator)
    return d

def evaluate_and_plot(model, X_test_rnn, y_test_rnn, scaler_y, filename_prefix):
    test_loss = model.evaluate(X_test_rnn, y_test_rnn, verbose=1)
    test_predictions = model.predict(X_test_rnn)

    # Convert to original scale
    test_predictions_original = scaler_y.inverse_transform(test_predictions)
    y_test_original = scaler_y.inverse_transform(y_test_rnn)

    # Use all data
    test_predictions_filtered = test_predictions_original
    y_test_filtered = y_test_original

    # Calculate Metrics using all data
    amb_value = amb(y_test_filtered.flatten(), test_predictions_filtered.flatten())
    mse = mean_squared_error(y_test_filtered, test_predictions_filtered)
    rmse = np.sqrt(mse)
    kge_value = kge(y_test_filtered.flatten(), test_predictions_filtered.flatten())
    nse_value = nse(y_test_filtered.flatten(), test_predictions_filtered.flatten())
    d = willmott_skill(y_test_filtered.flatten(), test_predictions_filtered.flatten())
    
    # Metrics text
    metrics_text = [f'RMSE: {rmse:.4f}', f'KGE: {kge_value:.4f}', f'NSE: {nse_value:.4f}', 
                    f"Willmott's d: {d:.4f}", f'mBias: {amb_value:.4f}']
    
    # Plotting using all data
    plt.figure(figsize=(10, 5))
    plt.plot(y_test_filtered, 'r--', label='Actual water level')
    plt.plot(test_predictions_filtered, label='Predicted water level')

    # Create dummy plots for metrics text
    for text in metrics_text:
        plt.plot([], [], ' ', label=text)

    plt.xlabel('Time (h)')
    plt.ylabel('Water level (m)')
    plt.title(f'Sandy hook, NJ ({prefix})')
    plt.legend(loc='best')

    plt.savefig(f"{location}/{prefix}.png", bbox_inches='tight', dpi=300)
#     plt.show()

    # One-to-One Plot using all data
    plt.figure(figsize=(6, 6))
    max_val = max(y_test_filtered.max(), test_predictions_filtered.max())
    min_val = min(y_test_filtered.min(), test_predictions_filtered.min())

    # One-to-one line
    plt.plot([min_val, max_val], [min_val, max_val], 'k--', label='One-to-one line')

    # Scatter plot of predictions vs actual
    plt.scatter(y_test_filtered, test_predictions_filtered, alpha=0.7, label='Predicted water level')
    plt.scatter(y_test_filtered, y_test_filtered, color='red', s=5, label='Actual water level')

    # Metrics text
    metrics_text = [f'RMSE: {rmse:.4f}', f'KGE: {kge_value:.4f}', f'NSE: {nse_value:.4f}', 
                    f"Willmott's d: {d:.4f}", f'mBias: {amb_value:.4f}']

    # Create dummy plots for metrics text
    for text in metrics_text:
        plt.plot([], [], ' ', label=text)

    plt.xlabel('Actual water level (m)')
    plt.ylabel('Predicted water level (m)')
    plt.title(f'Sandy hook, NJ ({prefix})')

    # Legend
    plt.legend(loc='best')

    # Save and show the plot
    plt.savefig(f"{location}/{prefix}_one_to_one.png", bbox_inches='tight', dpi=300)
    plt.show()

    return rmse, kge_value, nse_value, d, amb_value

def extract_lookback_from_filename(filename):
    # Find the position of 'h' in the filename
    h_position = filename.find('h')
    
    # Check if the character before 'h' is a digit
    if filename[h_position-1].isdigit():
        # If the character two positions before 'h' is also a digit, extract both digits
        if filename[h_position-2].isdigit():
            lookback = int(filename[h_position-2:h_position])
        else:
            lookback = int(filename[h_position-1:h_position])
    else:
        lookback = None

    return lookback

def extract_prefix_from_filename(filename):
    # Extract the prefix substring before ".h5" in the filename
    return filename.split(".h5")[0]

def extract_and_save_all(test_predictions_original, y_test_original, lookback, DateTime, prefix, filename):
    # Use all indices
    indices = np.arange(len(y_test_original.flatten()))
    
    # Modifying the column name for prediction data to be in lowercase, remove "lstm", and adding "cv" as a suffix
    prediction_column_name = prefix.replace("lstm", "")
    
    data_full = pd.DataFrame({
        'Actual water level': y_test_original.flatten()[indices],
        prediction_column_name: test_predictions_original.flatten()[indices]
    })
    
    # Adjust the indices for the lookback period
    indices_adjusted = [index + lookback for index in indices]
    
    # Extract the corresponding date and time from the DateTime dataframe
    dates = DateTime['Date'].iloc[indices_adjusted]
    times = DateTime['Time (GMT)'].iloc[indices_adjusted]
    data_full['Date'] = dates.values
    data_full['Time (GMT)'] = times.values
    
    # Convert 'Date' column to datetime64[ns] dtype
    data_full['Date'] = pd.to_datetime(data_full['Date'])
    
    # Rearrange columns using the modified column name
    data_full = data_full[['Date', 'Time (GMT)', 'Actual water level', prediction_column_name]]
    
    # If the file doesn't exist, create one
    if not os.path.exists(filename):
        data_full.to_csv(filename, index=False)
    else:
        # If file exists, load the existing data
        existing_data = pd.read_csv(filename)

        # Convert 'Date' column to datetime64[ns] dtype
        existing_data['Date'] = pd.to_datetime(existing_data['Date'])

        # Merge with the new prediction column, replacing the existing column if it exists
        existing_data = existing_data.drop(columns=[prediction_column_name], errors='ignore')
        existing_data = existing_data.merge(data_full[['Date', 'Time (GMT)', prediction_column_name]], 
                                    on=['Date', 'Time (GMT)'], how='left')

        existing_data.to_csv(filename, index=False)

    return data_full

# Get a list of all the files in the directory
all_files = os.listdir()

# Filter out the files that end with ".h5"
model_files = [file for file in all_files if file.endswith(".h5")]

# Extracting the lookbacks from the model filenames again
lookbacks = [extract_lookback_from_filename(model_file) for model_file in model_files]

# Extracting the prefixes from the model filenames
prefixes = [extract_prefix_from_filename(model_file) for model_file in model_files]

folder_name = 'Lewes'

# Check if the folder exists, and create it if it doesn't
if not os.path.exists(folder_name):
    os.makedirs(folder_name)

# Save the plot in the specified folder
file_path = os.path.join(folder_name, "DAT_EWL_SANDY1992.csv")

# Define the filename to save the data
filename = "Sandy/DAT_EWL_SANDY1992.csv"

# Run the loop again
for model_file, lookback, prefix in zip(model_files, lookbacks, prefixes):
    X_test_rnn, y_test_rnn = create_rnn_dataset(X_norm, y_scaled, lookback)
    X_test_rnn = X_test_rnn.transpose(0, 2, 1)

    # Load the model
    model = load_model(model_file, custom_objects={'CustomAttentionLayer': CustomAttentionLayer})

    # Evaluate and plot
    amb_value, rmse, kge_value, nse_value, d = evaluate_and_plot(model, X_test_rnn, y_test_rnn, scaler_y, prefix)

    # Extract and save all data using the updated function
    test_predictions_original = scaler_y.inverse_transform(model.predict(X_test_rnn))
    y_test_original = scaler_y.inverse_transform(y_test_rnn)
    DAT_EWL_SANDY1992 = extract_and_save_all(test_predictions_original, y_test_original, lookback, DateTime, prefix, filename)
